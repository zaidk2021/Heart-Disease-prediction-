# Required libraries
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import xgboost as xgb
from imblearn.over_sampling import ADASYN

# List of target variables
targets = ['atr_fib', 'supr_tach', 'ven_tach', 'ven_fib', 'th_d_av', 'pul_ede', 'myo_rup', 'dres_syn', 'ch_hf', 're_myin', 'p_inf_ang']

# Function to load and preprocess data
def load_and_preprocess_data(train_path, test_path):
    # Load datasets
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    replacements = {'nan': np.nan, 'none': np.nan, 'NaN': np.nan, '': np.nan, '?': np.nan}
    train_df.replace(replacements, inplace=True)
    test_df.replace(replacements, inplace=True)

    # Convert target variables from 'NO', 'YES' to 0, 1
    for target in targets:
        train_df[target] = train_df[target].map({'NO': 0, 'YES': 1})

    # Separate target variables
    train_targets_df = train_df[targets]
    train_features_df = train_df.drop(targets, axis=1)
    test_features_df = test_df.drop(targets, axis=1, errors='ignore')

    # Impute missing values
    numerical_cols = train_features_df.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = train_features_df.select_dtypes(include=['object', 'bool']).columns
    imputer_num = SimpleImputer(strategy='median')
    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')

    # Impute and transform training features
    train_features_df[numerical_cols] = imputer_num.fit_transform(train_features_df[numerical_cols])
    train_features_df[categorical_cols] = train_features_df[categorical_cols].astype(str)
    train_features_df[categorical_cols] = imputer_cat.fit_transform(train_features_df[categorical_cols])

    # Impute and transform testing features
    test_features_df[numerical_cols] = imputer_num.transform(test_features_df[numerical_cols])
    test_features_df[categorical_cols] = test_features_df[categorical_cols].astype(str)
    test_features_df[categorical_cols] = imputer_cat.transform(test_features_df[categorical_cols])

    return train_features_df, train_targets_df, test_features_df, numerical_cols, categorical_cols

# Function to encode features and apply ADASYN
def encode_and_balance_features(train_features_df, train_targets_df, test_features_df, categorical_cols):
    encoder = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)], remainder='passthrough')
    X_train_encoded = encoder.fit_transform(train_features_df)
    X_test_encoded = encoder.transform(test_features_df)
    adasyn = ADASYN(random_state=42)
    
    balanced_X_train = {}
    balanced_y_train = {}
    
    for target in targets:
        X_resampled, y_resampled = adasyn.fit_resample(X_train_encoded, train_targets_df[target])
        balanced_X_train[target] = X_resampled
        balanced_y_train[target] = y_resampled
    
    return balanced_X_train, balanced_y_train, X_test_encoded, encoder

# Prediction function using XGBoost and balanced data
def make_predictions_with_xgboost(balanced_X_train, balanced_y_train, X_test_encoded, targets, encoder):
    predictions = {}
    for target in targets:
        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
        model.fit(balanced_X_train[target], balanced_y_train[target])
        predictions[target] = model.predict(X_test_encoded)
    return predictions

# Main workflow
def main(train_path, test_path):
    # Load and preprocess data
    train_features_df, train_targets_df, test_features_df, numerical_cols, categorical_cols = load_and_preprocess_data(train_path, test_path)

    # Encode features and balance data
    balanced_X_train, balanced_y_train, X_test_encoded, encoder = encode_and_balance_features(train_features_df, train_targets_df, test_features_df, categorical_cols)

    # Since the balanced_X_train is a dictionary with targets as keys, choose one target's features for prediction
    # This is a simplification and might not apply well if targets are highly imbalanced differently
    chosen_target = targets[0] # Choosing the first target for demonstration
    X_train_balanced = balanced_X_train[chosen_target]

    # Make predictions with XGBoost using balanced data
    predictions = make_predictions_with_xgboost(balanced_X_train, balanced_y_train, X_test_encoded, targets, encoder)

    # Convert predictions to DataFrame
    predictions_df = pd.DataFrame(predictions)

    # Display the first few rows of the predictions DataFrame
    print(predictions_df.head())

    test_df=pd.read_csv('/test.csv')
    predictions_df['ID'] = test_df['ID'].values

# want 'ID' as the first column
    cols = ['ID'] + [col for col in predictions_df.columns if col != 'ID']
    predictions_df = predictions_df[cols]

    print(predictions_df.head())
    # Convert "NO" to 0 and "YES" to 1 for all columns except the ID column
    conversion_dict = {'NO': 0, 'YES': 1}
    df_transformed = predictions_df.replace(conversion_dict)
    # Convert 'ID' column to integer type (Int32)
    # Since IDs are represented as floating point numbers but we want them as integers, we'll convert them directly
    df_transformed['ID'] = df_transformed['ID'].astype('Int32')

    # Display the DataFrame to confirm the 'ID' column type conversion
    df_transformed.head()

    # Display the transformed DataFrame to confirm the changes
    print(df_transformed)

    df_transformed.to_csv('/home/outputadasyn.csv', index=False)


if __name__ == "__main__":
    train_path = '/train.csv' 
    test_path = '/test.csv' 
    main(train_path, test_path)
